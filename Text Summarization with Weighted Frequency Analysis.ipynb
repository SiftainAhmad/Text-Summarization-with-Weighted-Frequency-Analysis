{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization with Weighted Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: This text summarization is based on simply word's Weighted Frequency, So it can't capture semantics and it's just a simple mathematical summarized tool, which may or may not capture the summary everytime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUZSzuDKSfNp"
   },
   "source": [
    "STEPS FOLLOWED:::\n",
    "            \n",
    "Extract the paragraph, \n",
    "Convert the paragraph to sentences, \n",
    "Preprocessing: remove special character,stop words,numbers, \n",
    "Tokenization, \n",
    "Find weighted frequency of occurence:(Freq of ith word/freq of word having max freq), \n",
    "Calculated the weighted sum of the words in the sentence, \n",
    "Sorting sentences in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For web scraping purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPchbd11S7NL",
    "outputId": "9eabc0d2-2ede-4772-f4c7-803d59a5438b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sifta\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sifta\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  For parsing XML and HTML documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkAJLAtWUGZL",
    "outputId": "b92f4656-6028-4c84-c35a-e0d16b713875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\sifta\\anaconda3\\lib\\site-packages (4.9.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping the data from  a URL link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z0EBifC3UJts"
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "scrapped_data=urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Machine_learning\")\n",
    "article=scrapped_data.read()\n",
    "\n",
    "parsing=bs.BeautifulSoup(article,'lxml')\n",
    "paragraphs=parsing.find_all('p')\n",
    "\n",
    "text_article=\"\"\n",
    "for p in paragraphs:\n",
    "  text_article+=p.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "Rb6WfzcxU179",
    "outputId": "7637daaf-9295-4990-c750-899eef3c6d22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Recently, artificial neural networks have been able to surpass many previous approaches in performance.[2][3]\\nMachine learning approaches have been applied to many fields including natural language processing, computer vision, speech recognition, emai'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_article[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zrEaat1BVFlt"
   },
   "outputs": [],
   "source": [
    "text_article=re.sub(r'\\[[0-9]*\\]',' ',text_article)\n",
    "text_article=re.sub(r'\\s+',' ',text_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "r0yVOP_ZVd-t",
    "outputId": "76ddb826-d34f-40df-ed82-e4300d22fc42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance. Machine learning approaches have been applied to many fields including natural language processing, computer vision, speech recognition, email filteri'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_article[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EvPsuuh9Vf1W"
   },
   "outputs": [],
   "source": [
    "# Removed Punctuation and special characters.\n",
    "formatted_text_article=re.sub('[^a-zA-Z]',' ',text_article)\n",
    "formatted_text_article=re.sub('\\s+',' ',formatted_text_article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "j64NeDSOWBWa",
    "outputId": "186996bb-9ba1-43bb-eddc-9556ec0dd7e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning ML is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions Recently artificial neural networks have been able to surpass many previous approaches in performance Machine learning approaches have been applied to many fields including natural language processing computer vision speech recognition email filtering agricu'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_text_article[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FBfW0D5eWFxd",
    "outputId": "3483f5f0-128a-4305-cc34-0d1bdc6ad1bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sifta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentence_list=nltk.sent_tokenize(text_article) #taken this article so that we can make sentences based on  full stop(.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rv0kWd9DWv3g",
    "outputId": "49550029-22ea-4ce5-bb8e-55ba31814f1d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sifta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "word_frequencies={}\n",
    "for word in nltk.word_tokenize(formatted_text_article):\n",
    "  if word not in stopwords:\n",
    "    if word not in word_frequencies.keys():\n",
    "      word_frequencies[word]=1\n",
    "    else:\n",
    "      word_frequencies[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOr6bK1yXZv-",
    "outputId": "0c062b28-f671-4522-e6cf-b0e572a6def5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Machine | freq: 16\n",
      "Word: learning | freq: 192\n",
      "Word: ML | freq: 4\n",
      "Word: field | freq: 19\n",
      "Word: study | freq: 8\n",
      "Word: artificial | freq: 24\n",
      "Word: intelligence | freq: 10\n",
      "Word: concerned | freq: 4\n",
      "Word: development | freq: 1\n",
      "Word: statistical | freq: 9\n"
     ]
    }
   ],
   "source": [
    "# Convert dictionary items to list and get the first 10 items\n",
    "word_freq = list(word_frequencies.items())[:10]\n",
    "\n",
    "for sentence, score in word_freq:\n",
    "    print(f\"Word: {sentence} | freq: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Word Frequency to Weighted Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qR6LaxcZXk9k"
   },
   "outputs": [],
   "source": [
    "maximum_frequency=max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "  word_frequencies[word]=(word_frequencies[word]/maximum_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2BN1yiJX6uG",
    "outputId": "bfce5b05-4f70-4d17-c717-09855f5084f7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Machine | weighted freq: 0.08333333333333333\n",
      "Word: learning | weighted freq: 1.0\n",
      "Word: ML | weighted freq: 0.020833333333333332\n",
      "Word: field | weighted freq: 0.09895833333333333\n",
      "Word: study | weighted freq: 0.041666666666666664\n",
      "Word: artificial | weighted freq: 0.125\n",
      "Word: intelligence | weighted freq: 0.052083333333333336\n",
      "Word: concerned | weighted freq: 0.020833333333333332\n",
      "Word: development | weighted freq: 0.005208333333333333\n",
      "Word: statistical | weighted freq: 0.046875\n"
     ]
    }
   ],
   "source": [
    "# Convert dictionary items to list and get the first 10 items\n",
    "weighted_word = list(word_frequencies.items())[:10]\n",
    "\n",
    "for sentence, score in weighted_word:\n",
    "    print(f\"Word: {sentence} | weighted freq: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sentence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "42so2v-3YBug"
   },
   "outputs": [],
   "source": [
    "sentence_scores={}\n",
    "for sent in sentence_list:\n",
    "  for word in nltk.word_tokenize(sent.lower()):\n",
    "    if word in word_frequencies.keys():\n",
    "      if len(sent.split(' '))<30:\n",
    "        if sent not in sentence_scores.keys():\n",
    "          sentence_scores[sent]=word_frequencies[word]\n",
    "        else:\n",
    "          sentence_scores[sent]+=word_frequencies[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Recently, artificial neural networks have been able to surpass many previous approaches in performance. | Score: 0.5677083333333334\n",
      "Sentence: Machine learning approaches have been applied to many fields including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. | Score: 2.0312500000000004\n",
      "Sentence: ML is known in its application across business problems under the name predictive analytics. | Score: 0.203125\n",
      "Sentence: Although not all machine learning is statistically based, computational statistics is an important source of the field's methods. | Score: 1.9531249999999998\n",
      "Sentence: The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. | Score: 0.41145833333333337\n",
      "Sentence: Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning. | Score: 2.375\n",
      "Sentence: From a theoretical point of view Probably approximately correct (PAC) learning provides a framework for describing machine learning. | Score: 2.6302083333333335\n",
      "Sentence: The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. | Score: 1.9062499999999996\n",
      "Sentence: The synonym self-teaching computers was also used in this time period. | Score: 0.3958333333333333\n",
      "Sentence: In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. | Score: 0.3124999999999999\n"
     ]
    }
   ],
   "source": [
    "# Convert dictionary items to list and get the first 10 items\n",
    "top_10_sentences = list(sentence_scores.items())[:10]\n",
    "\n",
    "for sentence, score in top_10_sentences:\n",
    "    print(f\"Sentence: {sentence} | Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing heapq to sort the sentence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWLbIoA0ZS7b",
    "outputId": "48fc2355-e3e5-494c-f467-29dc437c198f"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "summary_sentences=heapq.nlargest(7,sentence_scores,key=sentence_scores.get)\n",
    "\n",
    "summary='\\n'.join(summary_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarized Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g.\n",
      "Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data).\n",
      "A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.\n",
      "When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data.\n",
      "Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\n",
      "Embedded Machine Learning could be applied through several techniques including hardware acceleration, using approximate computing, optimization of machine learning models and many more.\n",
      "Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR TEXT NOT FOR URLs\n",
    "# text_article=input('Enter the paragraph:\\n')\n",
    "# text_article=re.sub(r'\\[[0-9]*\\]',' ',text_article)\n",
    "# text_article=re.sub(r'\\s+',' ',text_article)\n",
    "# # Removed Punctuation and special characters.\n",
    "# formatted_text_article=re.sub('[^a-zA-Z]',' ',text_article)\n",
    "# formatted_text_article=re.sub('\\s+',' ',formatted_text_article)\n",
    "# import nltk\n",
    "# # nltk.download('punkt')\n",
    "# sentence_list=nltk.sent_tokenize(text_article) #taken this article so that we can make sentences based on  full stop(.)\n",
    "# # nltk.download('stopwords')\n",
    "\n",
    "# stopwords=nltk.corpus.stopwords.words('english')\n",
    "# word_frequencies={}\n",
    "# for word in nltk.word_tokenize(formatted_text_article):\n",
    "#   if word not in stopwords:\n",
    "#     if word not in word_frequencies.keys():\n",
    "#       word_frequencies[word]=1\n",
    "#     else:\n",
    "#       word_frequencies[word]+=1\n",
    "\n",
    "# maximum_frequency=max(word_frequencies.values())\n",
    "# for word in word_frequencies.keys():\n",
    "#   word_frequencies[word]=(word_frequencies[word]/maximum_frequency)\n",
    "\n",
    "# sentence_scores={}\n",
    "# for sent in sentence_list:\n",
    "#   for word in nltk.word_tokenize(sent.lower()):\n",
    "#     if word in word_frequencies.keys():\n",
    "#       if len(sent.split(' '))<30:\n",
    "#         if sent not in sentence_scores.keys():\n",
    "#           sentence_scores[sent]=word_frequencies[word]\n",
    "#         else:\n",
    "#           sentence_scores[sent]+=word_frequencies[word]\n",
    "\n",
    "# import heapq\n",
    "# num_sentences=int(input('\\nEnter number of sentences in which you want to summarize: '))\n",
    "# summary_sentences=heapq.nlargest(num_sentences,sentence_scores,key=sentence_scores.get)\n",
    "\n",
    "# summary='\\n'.join(summary_sentences)\n",
    "# print(\"\\n*****\\033[1m\\033[4m\\033[7mSUMMARIZATION:\\033[0m*****\\n\")\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Input URLs/ Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 'url' to summarize content from a URL, or 'text' to summarize your own paragraph: \n",
      "url\n",
      "Enter the URL to summarize: \n",
      "https://en.wikipedia.org/wiki/Education_in_India\n",
      "Enter number of sentences in which you want to summarize: \n",
      "10\n",
      "\n",
      "*****\u001b[1m\u001b[4m\u001b[7mSUMMARIZATION:\u001b[0m*****\n",
      "\n",
      "The policy is a comprehensive framework for elementary education to higher education as well as vocational training in both rural and urban India.\n",
      "[7]\n",
      "Education in India covers different levels and types of learning, such as early childhood education, primary education, secondary education, higher education, and vocational education.\n",
      "[159][160]\n",
      "The school education structure in India typically follows a 10+2 system, which consists of 10 years of primary and secondary education followed by two years of higher secondary education.\n",
      "These terms are widely used across the country to refer to the stage of education that follows primary education and precedes higher secondary education.\n",
      "The District Education Revitalisation Programme (DERP) was launched in 1994 with an aim to universalise primary education in India by reforming and vitalising the existing primary education system.\n",
      "Education from class 1 to 4 is classified as lower primary education (LP) and class 5 to 7 as upper primary (UP) education.\n",
      "It consists of ten years of primary and secondary education (up to the 10th grade) followed by two years of higher secondary education (11th and 12th grade).\n",
      "The Indian government lays emphasis on primary education (Class I-VIII) also referred to as elementary education, to children aged 6 to 14 years old.\n",
      "After completing secondary or high school education, students move on to higher secondary education, which includes classes XI and XII (grades 11â€“12).\n",
      "[207] Nursery schools, elementary schools, secondary school, and schools for adult education for women were set up.\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "import heapq\n",
    "\n",
    "def summarize_text(text, num_sentences):\n",
    "    formatted_text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    formatted_text = re.sub(r'\\s+', ' ', formatted_text)\n",
    "    formatted_text = re.sub('[^a-zA-Z]', ' ', formatted_text)\n",
    "    formatted_text = re.sub('\\s+', ' ', formatted_text)\n",
    "\n",
    "    sentence_list = nltk.sent_tokenize(text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    word_frequencies = {}\n",
    "\n",
    "    for word in nltk.word_tokenize(formatted_text):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "\n",
    "    maximum_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n",
    "\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    summary = '\\n'.join(summary_sentences)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def summarize_url(url, num_sentences):\n",
    "    scrapped_data = urllib.request.urlopen(url)\n",
    "    article = scrapped_data.read()\n",
    "\n",
    "    parsing = bs.BeautifulSoup(article, 'lxml')\n",
    "    paragraphs = parsing.find_all('p')\n",
    "\n",
    "    text_article = \"\"\n",
    "    for p in paragraphs:\n",
    "        text_article += p.text\n",
    "\n",
    "    summary = summarize_text(text_article, num_sentences)\n",
    "    return summary\n",
    "\n",
    "def main():\n",
    "    source = input(\"Enter 'url' to summarize content from a URL, or 'text' to summarize your own paragraph: \\n\")\n",
    "\n",
    "    if source.lower() == 'url':\n",
    "        url = input(\"Enter the URL to summarize: \\n\")\n",
    "        num_sentences = int(input(\"Enter number of sentences in which you want to summarize: \\n\"))\n",
    "        summary = summarize_url(url, num_sentences)\n",
    "    elif source.lower() == 'text':\n",
    "        text = input(\"Enter the paragraph to summarize: \\n\")\n",
    "        num_sentences = int(input(\"Enter number of sentences in which you want to summarize: \\n\"))\n",
    "        summary = summarize_text(text, num_sentences)\n",
    "    else:\n",
    "        print(\"Invalid input. Please enter 'url' or 'text'.\")\n",
    "\n",
    "    print(\"\\n*****\\033[1m\\033[4m\\033[7mSUMMARIZATION:\\033[0m*****\\n\")\n",
    "    print(summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 'url' to summarize content from a URL, or 'text' to summarize your own paragraph: \n",
      "text\n",
      "Enter the paragraph to summarize: \n",
      "Cricket is a popular sport played worldwide, particularly in countries like India, England, Australia, and South Africa. It's a bat-and-ball game played between two teams of eleven players each. The game is played on a grass field with a flat strip called the pitch at the center. The objective is to score runs by hitting the ball and running between two sets of wickets, while the opposing team tries to get the batsmen out. The game has different formats, including Test cricket, One Day Internationals (ODIs), and Twenty20 (T20) matches, each with its own set of rules and strategies. Test cricket is the longest format, lasting up to five days, while T20 matches are completed in a few hours. Cricket has a rich history dating back centuries, with its origins traced to medieval England. It has evolved over time, with innovations such as limited-overs cricket and the introduction of technology like the Decision Review System (DRS). The sport has produced legendary players like Sachin Tendulkar, Sir Don Bradman, and Brian Lara, who are celebrated for their skill and contribution to the game. Cricket has a massive following, with fans passionate about supporting their favorite teams and players. Major cricket tournaments like the ICC Cricket World Cup and the Indian Premier League (IPL) attract millions of viewers worldwide. Cricket has also become a symbol of national pride for many countries, with matches between rival nations often sparking intense rivalries and excitement among fans. The sport has a unique vocabulary, with terms like \"bouncer,\" \"googly,\" and \"lbw\" commonly used by players and commentators. Overall, cricket is not just a game but a cultural phenomenon that brings people together and ignites a spirit of camaraderie and competition.\n",
      "Enter number of sentences in which you want to summarize: \n",
      "6\n",
      "\n",
      "*****\u001b[1m\u001b[4m\u001b[7mSUMMARIZATION:\u001b[0m*****\n",
      "\n",
      "Cricket has also become a symbol of national pride for many countries, with matches between rival nations often sparking intense rivalries and excitement among fans.\n",
      "Cricket is a popular sport played worldwide, particularly in countries like India, England, Australia, and South Africa.\n",
      "The sport has produced legendary players like Sachin Tendulkar, Sir Don Bradman, and Brian Lara, who are celebrated for their skill and contribution to the game.\n",
      "Major cricket tournaments like the ICC Cricket World Cup and the Indian Premier League (IPL) attract millions of viewers worldwide.\n",
      "The sport has a unique vocabulary, with terms like \"bouncer,\" \"googly,\" and \"lbw\" commonly used by players and commentators.\n",
      "Overall, cricket is not just a game but a cultural phenomenon that brings people together and ignites a spirit of camaraderie and competition.\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "import heapq\n",
    "\n",
    "def summarize_text(text, num_sentences):\n",
    "    formatted_text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    formatted_text = re.sub(r'\\s+', ' ', formatted_text)\n",
    "    formatted_text = re.sub('[^a-zA-Z]', ' ', formatted_text)\n",
    "    formatted_text = re.sub('\\s+', ' ', formatted_text)\n",
    "\n",
    "    sentence_list = nltk.sent_tokenize(text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    word_frequencies = {}\n",
    "\n",
    "    for word in nltk.word_tokenize(formatted_text):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "\n",
    "    maximum_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n",
    "\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    summary = '\\n'.join(summary_sentences)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def summarize_url(url, num_sentences):\n",
    "    scrapped_data = urllib.request.urlopen(url)\n",
    "    article = scrapped_data.read()\n",
    "\n",
    "    parsing = bs.BeautifulSoup(article, 'lxml')\n",
    "    paragraphs = parsing.find_all('p')\n",
    "\n",
    "    text_article = \"\"\n",
    "    for p in paragraphs:\n",
    "        text_article += p.text\n",
    "\n",
    "    summary = summarize_text(text_article, num_sentences)\n",
    "    return summary\n",
    "\n",
    "def main():\n",
    "    source = input(\"Enter 'url' to summarize content from a URL, or 'text' to summarize your own paragraph: \\n\")\n",
    "\n",
    "    if source.lower() == 'url':\n",
    "        url = input(\"Enter the URL to summarize: \\n\")\n",
    "        num_sentences = int(input(\"Enter number of sentences in which you want to summarize: \\n\"))\n",
    "        summary = summarize_url(url, num_sentences)\n",
    "    elif source.lower() == 'text':\n",
    "        text = input(\"Enter the paragraph to summarize: \\n\")\n",
    "        num_sentences = int(input(\"Enter number of sentences in which you want to summarize: \\n\"))\n",
    "        summary = summarize_text(text, num_sentences)\n",
    "    else:\n",
    "        print(\"Invalid input. Please enter 'url' or 'text'.\")\n",
    "\n",
    "    print(\"\\n*****\\033[1m\\033[4m\\033[7mSUMMARIZATION:\\033[0m*****\\n\")\n",
    "    print(summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
